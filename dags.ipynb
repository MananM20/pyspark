{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzjSoYBNdPPVwiXDA8zdHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MananM20/pyspark/blob/main/dags.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JAOYvCdGKIt"
      },
      "outputs": [],
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from datetime import datetime\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 3, 9),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "}\n",
        "with DAG('calculator_dag', default_args=default_args, schedule_interval=None) as dag:\n",
        "    # Prompt users for operation\n",
        "    input_operation = BashOperator(\n",
        "        task_id='input_operation',\n",
        "        bash_command='read -p \"Enter operation (+, -, *, /): \" operation',\n",
        "    )\n",
        "    # Prompt users for input numbers\n",
        "    input_x = BashOperator(\n",
        "        task_id='input_x',\n",
        "        bash_command='read -p \"Enter first number (x): \" x',\n",
        "    )\n",
        "    input_y = BashOperator(\n",
        "        task_id='input_y',\n",
        "        bash_command='read -p \"Enter second number (y): \" y',\n",
        "    )\n",
        "    # Perform calculation based on selected operation\n",
        "    calculate = BashOperator(\n",
        "        task_id='perform_calculation',\n",
        "        bash_command='echo \"The result is: $(($x $operation $y))\"',\n",
        "    )\n",
        "    # Define task dependencies\n",
        "    input_operation >> [input_x, input_y] >> calculate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.dummy_operator import DummyOperator\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 3, 11),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "}\n",
        "\n",
        "with DAG('calculator_dag_auto', default_args=default_args, schedule_interval=\"*/5 * * * *\") as dag:\n",
        "    # Set the operation and numbers as environment variables\n",
        "    # Prompt users for operation (simulated by setting an environment variable)\n",
        "    input_operation = DummyOperator(\n",
        "        task_id='input_operation',\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    # Prompt users for input numbers (simulated by setting environment variables)\n",
        "    input_x = DummyOperator(\n",
        "        task_id='input_x',\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    input_y = DummyOperator(\n",
        "        task_id='input_y',\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    # Perform calculation based on selected operation\n",
        "    calculate = DummyOperator(\n",
        "        task_id='perform_calculation',\n",
        "        dag=dag\n",
        "    )\n",
        "\n",
        "    # Define task dependencies\n",
        "    input_operation >> [input_x, input_y] >> calculate\n"
      ],
      "metadata": {
        "id": "OWwhbKc5Y90T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "def load_data():\n",
        "    # Load your dataset here\n",
        "    data = pd.read_csv('your_dataset.csv')\n",
        "    return data\n",
        "def preprocess_data(data):\n",
        "    # Perform data preprocessing here\n",
        "    # For example, handle missing values, encode categorical variables, etc.\n",
        "    return preprocessed_data\n",
        "def train_model(preprocessed_data):\n",
        "    # Split data into features and target\n",
        "    X = preprocessed_data.drop(columns=['target_column'])\n",
        "    y = preprocessed_data['target_column']\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    # Train a machine learning model\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "def save_model(model):\n",
        "    # Save the trained model to a file\n",
        "    joblib.dump(model, 'trained_model.pkl')\n",
        "    print(\"Model saved successfully.\")\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 3, 9),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "}\n",
        "with DAG('ml_pipeline_dag', default_args=default_args, schedule_interval=None) as dag:\n",
        "    load_data_task = PythonOperator(\n",
        "        task_id='load_data',\n",
        "        python_callable=load_data,\n",
        "    )\n",
        "    preprocess_data_task = PythonOperator(\n",
        "        task_id='preprocess_data',\n",
        "        python_callable=preprocess_data,\n",
        "        provide_context=True,\n",
        "    )\n",
        "    train_model_task = PythonOperator(\n",
        "        task_id='train_model',\n",
        "        python_callable=train_model,\n",
        "        provide_context=True,\n",
        "    )\n",
        "    save_model_task = PythonOperator(\n",
        "        task_id='save_model',\n",
        "        python_callable=save_model,\n",
        "        provide_context=True,\n",
        "    )\n",
        "    load_data_task >> preprocess_data_task >> train_model_task >> save_model_task"
      ],
      "metadata": {
        "id": "5xiBkTP4ZJst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.dummy_operator import DummyOperator\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "def load_data():\n",
        "    # Load your dataset here\n",
        "    data = pd.read_csv('your_dataset.csv')\n",
        "    return data\n",
        "def preprocess_data(data):\n",
        "    # Perform data preprocessing here\n",
        "    # For example, handle missing values, encode categorical variables, etc.\n",
        "    return preprocessed_data\n",
        "def train_model(preprocessed_data):\n",
        "    # Split data into features and target\n",
        "    X = preprocessed_data.drop(columns=['target_column'])\n",
        "    y = preprocessed_data['target_column']\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    # Train a machine learning model\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "def save_model(model):\n",
        "    # Save the trained model to a file\n",
        "    joblib.dump(model, 'trained_model.pkl')\n",
        "    print(\"Model saved successfully.\")\n",
        "def do_nothing():\n",
        "    pass\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 3, 9),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "}\n",
        "with DAG('ml_pipeline_dag_auto', default_args=default_args, schedule_interval=\"*/5 * * * *\") as dag:\n",
        "    load_data_task = PythonOperator(\n",
        "        task_id='load_data',\n",
        "        python_callable=do_nothing,\n",
        "        dag=dag,\n",
        "    )\n",
        "    preprocess_data_task = PythonOperator(\n",
        "        task_id='preprocess_data',\n",
        "        python_callable=do_nothing,\n",
        "        dag=dag,\n",
        "    )\n",
        "    train_model_task = PythonOperator(\n",
        "        task_id='train_model',\n",
        "        python_callable=do_nothing,\n",
        "        dag=dag,\n",
        "    )\n",
        "    save_model_task = PythonOperator(\n",
        "        task_id='save_model',\n",
        "        python_callable=do_nothing,\n",
        "        dag=dag,\n",
        "    )\n",
        "    load_data_task >> preprocess_data_task >> train_model_task >> save_model_task"
      ],
      "metadata": {
        "id": "enBoy683ZTid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ML Pipeline DAG\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def load_data():\n",
        "    # Load your dataset here\n",
        "    data = spark.read.csv('https://caps-newbucket.s3.amazonaws.com/Automobile.csv', header=True, inferSchema=True)\n",
        "    return data\n",
        "\n",
        "def preprocess_data(**kwargs):\n",
        "    ti = kwargs['ti']\n",
        "    data = ti.xcom_pull(task_ids='load_data')\n",
        "    # Perform data preprocessing here\n",
        "    # For example, handle missing values, encode categorical variables, etc.\n",
        "    # Here, we'll simply assemble features into a single vector\n",
        "    feature_cols = data.columns[:-1]  # Assuming last column is the target\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "    preprocessed_data = assembler.transform(data).select('features', 'target_column')\n",
        "    return preprocessed_data\n",
        "\n",
        "def train_model(**kwargs):\n",
        "    ti = kwargs['ti']\n",
        "    preprocessed_data = ti.xcom_pull(task_ids='preprocess_data')\n",
        "    # Train a machine learning model\n",
        "    rf = RandomForestClassifier()\n",
        "    model = rf.fit(preprocessed_data)\n",
        "    return model\n",
        "\n",
        "def save_model(**kwargs):\n",
        "    ti = kwargs['ti']\n",
        "    model = ti.xcom_pull(task_ids='train_model')\n",
        "    # Save the trained model to a file\n",
        "    model.write().save('hdfs://path/to/trained_model')  # Save model to HDFS or any other file system\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 3, 11),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "}\n",
        "\n",
        "with DAG('pyspark_ml_pipeline_dag', default_args=default_args, schedule_interval=None) as dag:\n",
        "    load_data_task = PythonOperator(\n",
        "        task_id='load_data',\n",
        "        python_callable=load_data,\n",
        "    )\n",
        "\n",
        "    preprocess_data_task = PythonOperator(\n",
        "        task_id='preprocess_data',\n",
        "        python_callable=preprocess_data,\n",
        "    )\n",
        "\n",
        "    train_model_task = PythonOperator(\n",
        "        task_id='train_model',\n",
        "        python_callable=train_model,\n",
        "    )\n",
        "\n",
        "    save_model_task = PythonOperator(\n",
        "        task_id='save_model',\n",
        "        python_callable=save_model,\n",
        "    )\n",
        "\n",
        "    load_data_task >> preprocess_data_task >> train_model_task >> save_model_task\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "JXyAmBSLZ4Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ML Pipeline DAG\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def load_data():\n",
        "    # Load your dataset here\n",
        "    data = spark.read.csv('https://caps-newbucket.s3.amazonaws.com/Automobile.csv', header=True, inferSchema=True)\n",
        "    return data\n",
        "\n",
        "def preprocess_data(**kwargs):\n",
        "    ti = kwargs['ti']\n",
        "    data = ti.xcom_pull(task_ids='load_data')\n",
        "    # Perform data preprocessing here\n",
        "    # For example, handle missing values, encode categorical variables, etc.\n",
        "    # Here, we'll simply assemble features into a single vector\n",
        "    feature_cols = data.columns[:-1]  # Assuming last column is the target\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
        "    preprocessed_data = assembler.transform(data).select('features', 'target_column')\n",
        "    return preprocessed_data\n",
        "\n",
        "def train_model(**kwargs):\n",
        "    ti = kwargs['ti']\n",
        "    preprocessed_data = ti.xcom_pull(task_ids='preprocess_data')\n",
        "    # Train a machine learning model\n",
        "    rf = RandomForestClassifier()\n",
        "    model = rf.fit(preprocessed_data)\n",
        "    return model\n",
        "def do_nothing():\n",
        "    pass\n",
        "\n",
        "def save_model(**kwargs):\n",
        "    ti = kwargs['ti']\n",
        "    model = ti.xcom_pull(task_ids='train_model')\n",
        "    # Save the trained model to a file\n",
        "    model.write().save('hdfs://path/to/trained_model')  # Save model to HDFS or any other file system\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 3, 11),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "}\n",
        "\n",
        "with DAG('pyspark_ml_pipeline_dag_auto', default_args=default_args, schedule_interval=\"*/5 * * * *\") as dag:\n",
        "    load_data_task = PythonOperator(\n",
        "        task_id='load_data',\n",
        "        python_callable=do_nothing,\n",
        "    )\n",
        "\n",
        "    preprocess_data_task = PythonOperator(\n",
        "        task_id='preprocess_data',\n",
        "        python_callable=do_nothing,\n",
        "    )\n",
        "\n",
        "    train_model_task = PythonOperator(\n",
        "        task_id='train_model',\n",
        "        python_callable=do_nothing,\n",
        "    )\n",
        "\n",
        "    save_model_task = PythonOperator(\n",
        "        task_id='save_model',\n",
        "        python_callable=do_nothing,\n",
        "    )\n",
        "\n",
        "    load_data_task >> preprocess_data_task >> train_model_task >> save_model_task\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "FgvizwHzjmBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
        "import requests\n",
        "import json\n",
        "def fetch_json(url):\n",
        "   \"\"\"Fetch JSON data from a given URL.\"\"\"\n",
        "   print(\"Fetching JSON data...\")\n",
        "   response = requests.get(url=url)\n",
        "   if response.status_code == 200:\n",
        "       return response.text\n",
        "   else:\n",
        "       raise Exception(f\"Failed to fetch data, status code: {response.status_code}\")\n",
        "def upload_json_to_s3(bucket_name, file_name, json_data):\n",
        "   \"\"\"Upload JSON data to an S3 bucket.\"\"\"\n",
        "   print(\"Uploading JSON data to S3...\")\n",
        "   key_path = f\"asset/{file_name}.json\"\n",
        "   hook = S3Hook('s3_conn')\n",
        "   hook.load_string(\n",
        "       string_data=json_data,\n",
        "       key=key_path,\n",
        "       bucket_name=bucket_name,\n",
        "       replace=True\n",
        "   )\n",
        "   print(f\"Success: JSON object has been uploaded to {key_path} S3 key path.\")\n",
        "default_args = {\n",
        "   'owner': 'airflow',\n",
        "   'start_date': datetime(2023, 9, 1),\n",
        "   'retries': 1,\n",
        "   'retry_delay': timedelta(minutes=5),\n",
        " }\n",
        "dag = DAG(\n",
        "   dag_id='dump_to_s3',\n",
        "   default_args=default_args,\n",
        "   schedule_interval=None,\n",
        "   catchup=False\n",
        " )\n",
        "fetch_json_task = PythonOperator(\n",
        "   task_id=\"fetch_json\",\n",
        "   python_callable=fetch_json,\n",
        "   op_args=[\"http://engineering-exam.s3-website.ap-southeast-2.amazonaws.com/\"],\n",
        "   dag=dag\n",
        " )\n",
        "upload_json_task = PythonOperator(\n",
        "   task_id=\"upload_json_to_s3\",\n",
        "   python_callable=upload_json_to_s3,\n",
        "   op_kwargs={\n",
        "       'bucket_name': \"INSERT YOUR S3 BUCKET NAME\",\n",
        "       'file_name': \"airflow_tester\",\n",
        "       'json_data': \"{{ ti.xcom_pull(task_ids='fetch_json') }}\"\n",
        "   },\n",
        "   dag=dag\n",
        " )\n",
        "fetch_json_task >> upload_json_task"
      ],
      "metadata": {
        "id": "AwxFIuBlj4cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import Dataset\n",
        "from airflow.decorators import dag, task\n",
        "from pendulum import datetime\n",
        "from airflow.providers.amazon.aws.operators.s3 import (\n",
        "    S3ListPrefixesOperator,\n",
        "    S3CreateObjectOperator,\n",
        ")\n",
        "MY_S3_BUCKET = \"myexamplebucketone\"\n",
        "MY_S3_FOLDER_PREFIX = \"e\"\n",
        "MY_S3_BUCKET_DELIMITER = \"/\"\n",
        "MY_DATA = \"Hi S3 bucket!\"\n",
        "MY_FILENAME = \"my_message.txt\"\n",
        "AWS_CONN_ID = \"aws_conn\"\n",
        "@dag(\n",
        "    dag_id=\"upstream_datasets_taskflow_usecase_pgs\",\n",
        "    start_date=datetime(2024, 3, 11),\n",
        "    schedule=None,\n",
        "    catchup=False,\n",
        "    tags=[\"datasets\", \"taskflow\", \"usecase\"],\n",
        ")\n",
        "def upstream_datasets_taskflow_usecase():\n",
        "    # list all root level folders in my S3 bucket that start with my prefix\n",
        "    list_folders_in_bucket = S3ListPrefixesOperator(\n",
        "        task_id=\"list_folders_in_bucket\",\n",
        "        aws_conn_id=AWS_CONN_ID,\n",
        "        bucket=MY_S3_BUCKET,\n",
        "        prefix=MY_S3_FOLDER_PREFIX,\n",
        "        delimiter=MY_S3_BUCKET_DELIMITER,\n",
        "    )\n",
        "    # create a separate Dataset object for each of the folders\n",
        "    @task\n",
        "    def create_datasets_from_s3_folders(folder_list):\n",
        "        datasets_lists = []\n",
        "        for folder in folder_list:\n",
        "            uri = f\"s3://{MY_S3_BUCKET}{MY_S3_BUCKET_DELIMITER}{folder}{MY_FILENAME}\"\n",
        "            datasets_lists.append(Dataset(uri))\n",
        "        print(f\"These datasets were created: {datasets_lists}\")\n",
        "        # returning a list of Datasets (2.5 feature)\n",
        "        return datasets_lists\n",
        "    list_of_datasets = create_datasets_from_s3_folders(list_folders_in_bucket.output)\n",
        "    # write MY_DATA into a new MY_FILENAME in each of the S3 folders\n",
        "    write_file_to_S3 = S3CreateObjectOperator.partial(\n",
        "        task_id=\"write_file_to_S3\", aws_conn_id=AWS_CONN_ID, data=MY_DATA, replace=True\n",
        "    ).expand(\n",
        "        s3_key=list_of_datasets.map(\n",
        "            lambda x: x.uri\n",
        "        )  # retrieving the keys from the Datasets\n",
        "    )\n",
        "    # since outlets is not a mappable parameter, use a task flow task to produce to the datasets\n",
        "    # this too is only possible in 2.5+\n",
        "    @task\n",
        "    def produce_to_datasets(dataset_obj):\n",
        "        return dataset_obj\n",
        "    write_file_to_S3 >> produce_to_datasets.expand(dataset_obj=list_of_datasets)\n",
        "upstream_datasets_taskflow_usecase()"
      ],
      "metadata": {
        "id": "hGJUJ3FOnIhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Upload Data to S3\n",
        "\n",
        "A simple DAG that shows how to upload data to S3.\n",
        "\n",
        "Creates 5 small CSV files by looping through a Python Operator and uploads them all to S3.\n",
        "This pattern of dynamically generating tasks can be used anytime the list of things in your DAGs is a known.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from airflow import DAG\n",
        "from airflow.operators.dummy_operator import DummyOperator\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "\n",
        "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "\n",
        "S3_CONN_ID='group7'\n",
        "BUCKET='caps-newbucket'\n",
        "\n",
        "name='workshop' #swap your name here\n",
        "\n",
        "\n",
        "def upload_to_s3(file_name):\n",
        "\n",
        "    # Instanstiate\n",
        "    s3_hook=S3Hook(aws_conn_id=S3_CONN_ID)\n",
        "\n",
        "    # Create file\n",
        "    sample_file = \"{0}_file_{1}.txt\".format(name, file_name) #swap your name here\n",
        "    example_file = open(sample_file, \"w+\")\n",
        "    example_file.write(\"Putting some data in for task {0}\".format(file_name))\n",
        "    example_file.close()\n",
        "\n",
        "    s3_hook.load_file(sample_file, 'globetelecom/{0}'.format(sample_file), bucket_name=BUCKET, replace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Default settings applied to all tasks\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "\n",
        "# Using a DAG context manager, you don't have to specify the dag property of each task\n",
        "with DAG('s3_upload',\n",
        "         start_date=datetime(2024, 3, 12),\n",
        "         max_active_runs=1,\n",
        "         schedule_interval='0 12 8-14,22-28 * 6',  # https://airflow.apache.org/docs/stable/scheduler.html#dag-runs\n",
        "         default_args=default_args,\n",
        "         catchup=False # enable if you don't want historical dag runs to run\n",
        "         ) as dag:\n",
        "\n",
        "    t0 = DummyOperator(task_id='start')\n",
        "\n",
        "    for i in range(0,5): # generates 10 tasks\n",
        "        generate_files=PythonOperator(\n",
        "            task_id='generate_file_{0}_{1}'.format(name, i), # task id is generated dynamically\n",
        "            python_callable=upload_to_s3,\n",
        "            op_kwargs= {'file_name': i}\n",
        "        )\n",
        "\n",
        "        t0 >> generate_files"
      ],
      "metadata": {
        "id": "YwFp49WJoUCE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}